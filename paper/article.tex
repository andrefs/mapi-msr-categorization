% This template has been tested with LLNCS DOCUMENT CLASS -- version 2.20 (24-JUN-2015)

% !TeX spellcheck = en-US
% !TeX encoding = utf8
% !TeX program = pdflatex
% !BIB program = bibtex
% -*- coding:utf-8 mod:LaTeX -*-

%"runningheads" enables:
%  - page number on page 2 onwards
%  - title/authors on even/odd pages
%This is good for other readers to enable proper archiving among other papers and pointing to
%content. Even if the title page states the title, when printed and stored in a folder, when
%blindly opening the folder, one could hit not the title page, but an arbitrary page. Therefore,
%it is good to have title printed on the pages, too.
\documentclass[runningheads,a4paper]{llncs}[2015/06/24]

%cmap has to be loaded before any font package (such as cfr-lm)
\usepackage{cmap}

\iftrue % use default-font
  %better font, similar to the default springer font
  %cfr-lm is preferred over lmodern. Reasoning at http://tex.stackexchange.com/a/247543/9075
  \usepackage[%
    rm={oldstyle=false,proportional=true},%
    sf={oldstyle=false,proportional=true},%
    tt={oldstyle=false,proportional=true,variable=true},%
    qt=false%
  ]{cfr-lm}
\else
  %
  %if more space is needed, exchange cfr-lm by two packages from newtx:
  \usepackage{newtxtext}
  \usepackage{newtxmath}
  \usepackage[zerostyle=b,scaled=.9]{newtxtt}
\fi

%for demonstration purposes only
\usepackage[math]{blindtext}

%% If you need packages for other papers,
%% START COPYING HERE
%%
%% COPY ALSO cmap from line 19 - it has to be loaded as the first package

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} %support umlauts in the input

\usepackage{graphicx}

%Set English as language and allow to write hyphenated"=words
%
%Even though `american`, `english` and `USenglish` are synonyms for babel package (according to https://tex.stackexchange.com/questions/12775/babel-english-american-usenglish), the llncs document class is prepared to avoid the overriding of certain names (such as "Abstract." -> "Abstract" or "Fig." -> "Figure") when using `english`, but not when using the other 2.
%english has to go last to set it as default language
\usepackage[ngerman,english]{babel}
%Hint by http://tex.stackexchange.com/a/321066/9075 -> enable "= as dashes
\addto\extrasenglish{\languageshorthands{ngerman}\useshorthands{"}}

%Sorts the citations in the brackets
%It also allows \cite{refa, refb}. Otherwise, the document does not compile.
%  Error message: "White space in argument"
\usepackage{cite}

% backticks (`) are rendered as such in verbatim environment. See https://tex.stackexchange.com/a/341057/9075 for details.
\usepackage{upquote}

%extended enumerate, such as \begin{compactenum}
\usepackage{paralist}

%put figures inside a text
%\usepackage{picins}
%use
%\piccaptioninside
%\piccaption{...}
%\parpic[r]{\includegraphics ...}
%Text...

%for easy quotations: \enquote{text}
\usepackage{csquotes}

%enable margin kerning
\RequirePackage{iftex}
\ifPDFTeX
  \RequirePackage[%
    final,%
    tracking=smallcaps,%
    expansion=alltext,%
    protrusion=alltext-nott]{microtype}%
\else
  \RequirePackage[%
    final,%
    protrusion=alltext-nott]{microtype}%
\fi%

%tweak \url{...}
\usepackage{url}
%\urlstyle{same}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
%nicer // - solution by http://tex.stackexchange.com/a/98470/9075
%DO NOT ACTIVATE -> prevents line breaks
%\makeatletter
%\def\Url@twoslashes{\mathchar`\/\@ifnextchar/{\kern-.2em}{}}
%\g@addto@macro\UrlSpecials{\do\/{\Url@twoslashes}}
%\makeatother

%diagonal lines in a table - http://tex.stackexchange.com/questions/17745/diagonal-lines-in-table-cell
%slashbox is not available in texlive (due to licensing) and also gives bad results. This, we use diagbox
%\usepackage{diagbox}

%required for pdfcomment later
\usepackage{xcolor}

%for listings
\usepackage{listings}
\lstset{%
  basicstyle=\ttfamily,%
  columns=fixed,%
  basewidth=.5em,%
  xleftmargin=0.5cm,%
  literate={á}{{\'a}}1 {ê}{{\^e}}1 {é}{{\'e}}1,
  captionpos=b}%
\renewcommand{\lstlistingname}{List.}
%Fix counter as described at https://tex.stackexchange.com/a/28334/9075
\usepackage{chngcntr}
\AtBeginDocument{\counterwithout{lstlisting}{section}}

%compatibility of packages minted and listings with respect to the numbering of "List." caption
%source: https://tex.stackexchange.com/a/269510/9075
\AtBeginEnvironment{listing}{\setcounter{listing}{\value{lstlisting}}}
\AtEndEnvironment{listing}{\stepcounter{lstlisting}}

%enable nice comments
%this also loads hyperref
\usepackage{pdfcomment}
%enable hyperref without colors and without bookmarks
\hypersetup{hidelinks,
  colorlinks=true,
  allcolors=black,
  pdfstartview=Fit,
  breaklinks=true}
%enables correct jumping to figures when referencing
\usepackage[all]{hypcap}

\newcommand{\commentontext}[2]{\colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{\pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

%compatibality with packages todo, easy-todo, todonotes
\newcommand{\todo}[1]{\commentatside{#1}}
%compatiblity with package fixmetodonotes
\newcommand{\TODO}[1]{\commentatside{#1}}

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}
\crefname{listing}{\lstlistingname}{\lstlistingname}
\Crefname{listing}{Listing}{Listings}

%\usepackage[newfloat]{minted}

%define IfPackageLoaded
\usepackage{ltxcmds}
\makeatletter
\newcommand{\IfPackageLoaded}[2]{\ltx@ifpackageloaded{#1}{#2}{}}
\makeatother

\IfPackageLoaded{minted}{
  % Line numbers not flowing out of the margin
  \setminted{numbersep=5pt, xleftmargin=12pt}

  %http://www.jevon.org/wiki/Eclipse_Pygments_Style
  %\usemintedstyle{eclipse}
  %
  %\usemintedstyle{autumn}
  %\usemintedstyle{rrt}
  %\usemintedstyle{borland}
  %\usemintedstyle{friendlygrayscale}
  \usemintedstyle{friendly}

  % We need to load caption to have a bold font on the label
  % The other parameters mimic the layout of the LNCS class
  \usepackage[labelfont=bf,font=small,skip=4pt]{caption}
  \SetupFloatingEnvironment{listing}{name=List.,within=none}
}{
}
%
%Following definitions are outside of IfPackageLoaded; inside, they are not visible
%
%Intermediate solution for hyperlined refs. See https://tex.stackexchange.com/q/132420/9075 for more information.
\newcommand{\Vlabel}[1]{\label[line]{#1}\hypertarget{#1}{}}
\newcommand{\lref}[1]{\hyperlink{#1}{\FancyVerbLineautorefname~\ref*{#1}}}

%If minted is not loaded, provide the environments nevertheless
\ifcsmacro{listing}{}{
  \newenvironment{listing}[1][htbp!]{\begin{figure}[#1]}{\end{figure}}
  \newcounter{listing}
}
\ifcsmacro{minted}{}{%
  \newenvironment{minted}[1][]{
    \bfseries
    Package minted was not loaded, so there is no XML code shown.

    In case you load minted, please be sure to \begin{enumerate}[a)]
      \item Have \href{https://www.python.org/downloads/}{python} and \href{http://pygments.org/download/}{pygments} installed
      \item Excecute pdflatex using \texttt{-shell-escape}
    \end{enumerate}
  }{}
}

\usepackage{xspace}
%\newcommand{\eg}{e.\,g.\xspace}
%\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }

%introduce \powerset - hint by http://matheplanet.com/matheplanet/nuke/html/viewtopic.php?topic=136492&post_id=997377
\DeclareFontFamily{U}{MnSymbolC}{}
\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
\DeclareFontShape{U}{MnSymbolC}{m}{n}{
  <-6>    MnSymbolC5
  <6-7>   MnSymbolC6
  <7-8>   MnSymbolC7
  <8-9>   MnSymbolC8
  <9-10>  MnSymbolC9
  <10-12> MnSymbolC10
  <12->   MnSymbolC12%
}{}
\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%% END COPYING HERE

\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

\title{Topic categorization in Portuguese news articles}
%If Title is too long, use \titlerunning
%\titlerunning{Short Title}

%Single insitute
\author{André F. Santos}
%If there are too many authors, use \authorrunning
%\authorrunning{First Author et al.}
\institute{\textit{CRACS \& INESC-Porto LA} \\ \textit{Faculty
of Sciences, University of Porto}\\ Porto, Portugal \\
afs@inesctec.pt}


%% Multiple insitutes - ALTERNATIVE to the above
% \author{%
%     Firstname Lastname\inst{1} \and
%     Firstname Lastname\inst{2}
% }
%
%If there are too many authors, use \authorrunning
%  \authorrunning{First Author et al.}
%
%  \institute{
%      Insitute 1\\
%      \email{...}\and
%      Insitute 2\\
%      \email{...}
%}

\maketitle

\begin{abstract}
Categorizing news articles according to their contents allows to
decrease the information entropy in a world where the rate of
publication of digital text documents is increasing fast. In this
article we describe ongoing work which aims to evaluate the
feasibility of implementing a classifier which is lightweight
enough to be used in real time on the client side of a web
application.  More specifically, we gathered a corpus of Portuguese
news and used it to train and evaluate several classification
algorithms. We analyze the results obtained in terms of the
classifiers error rate, training time and memory footprint.
\end{abstract}

\begin{keywords}
topic categorization, machine learning, text mining
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Online news articles first appeared as reprints from traditional
newspapers; nowadays, however, they represent now the primary source
of news for some segments of the population, both in developed and
developing countries (whether consumed directly in the newspaper
website, or indirectly e.g. through a social media application or a
feed catcher)\cite{greer2006evolution,boczkowski2005digitizing,chyi1999access}.

Unofficially known as \emph{the fourth branch of government}, the press
plays a vital role within our society, keeping us informed about
the current state of affairs (at a local and global scale) and acting
as a watchdog for the other three branches (legislative, executive and
judicial). The (lack of) freedom of press and access to the news in a
given country is even often considered an indicator of a lack of
democracy\cite{goode2009social,house2009freedom}.

As such, improving the ways citizens can access the information (view
it, query it and search for it) contained in news articles has the
potential to contribute for a more informed and, ultimately, better,
society\cite{bollinger1988tolerant}.

On the other hand, the last decades have witnessed a fast increase on
the rate of publication of digital text documents. Traditional
document types, such as news articles, scientific papers or books are
now published online along with new formats, such as blog posts or
tweets, each having thousands or millions of new documents published
each day\cite{hilbert2011world,allan2006online}.

Publication is not the only step which has moved to the digital world;
in fact, most often nowadays the whole document lifecycle happens
digitally, with virtual tools available for
researching, writing, styling, publishing and sharing\cite{williams2009personal}.

Having the entire workflow happening within the digital world presents
some opportunities when compared to the traditional
process\cite{o1997comparison}. In particular, due to the current
processing power commonly available, tasks related to the manipulation
of the information contained within these documents (searching,
compiling, annotating, sharing, \dots) can now be performed
automatically and targeting a large amount of articles.

In addition to the document content (for example, in a news article,
the \emph{title}, \emph{lead} and \emph{body}), its metadata is also
important: author(s), date of publication, source, topic, mentioned
entities and their relations, etc\cite{yaginuma2003metadata,yaginuma2003design}. Some of
this metadata might be filled in and stored along with the document
(e.g. \emph{author} and \emph{date of publication}); other is usually
extracted from the document content (e.g. mentioned
entities)\cite{vadrevu2005automated}.

An example of a feature which improves information access is the
categorization of news articles by the topic (or topics) of its
content\cite{kim2006extracting}. The presence of such a categorization
may influence the way the information is stored, organized, displayed
and queried\cite{teitler2008newsstand}.

\begin{figure}[H]
\centerline{\includegraphics[width=.67\columnwidth]{imgs/wireframe}}
\caption{Category classification and suggestion on the client side}
\label{fig:wireframe}
\end{figure}


The simplest way of achieving this categorization is to have the
author of the article manually introducing it (e.g. the journalist
typing it on the news article authoring framework); however, this
solution presents some challenges:
\begin{itemize}
    \item It increases the amount of work the author has to do.
    \item The author might not be sure which categories are available.
    \item The author might not be sure which category is the best
        (e.g. \emph{Economics} vs  \emph{Finance}).
    \item It does not scale -- e.g. if the goal is to categorize an
        existing (large) corpus.
\end{itemize}

Thus, an automated way of categorizing news articles could solve some
of these problems and decrease the burden of this task. Additionally,
a lightweight version of such a classifier could be implemented on the
client side code of a web application, for example, allowing the
categorization to happen in real time (i.e. as the author types in the
article text). Figure~\ref{fig:wireframe} presents a suggestion of
how this feature could look like if implemented on a web application.

The challenges of document classification have been well studied
within the machine learning research field of
study\cite{borko1963automatic,sebastiani2002machine,rubin2012statistical}.
Given a corpus of already classified documents, several algorithms
might be applied to train a classifier capable of determining the
category of additional articles.

In this article, we describe the preliminary results obtained in
developing a classifier to categorize news articles using a previously
manually categorized corpus. Additionally, we evaluate the possibility
of implementing such a classifier as lightweight as possible to allow
it to run on the client side of a web application.

\section{Methods}
In order to train and evaluate classification algorithms, we first
needed to choose and obtain a suitable dataset. Preferably, this
dataset should contain documents which were previously categorized,
allowing us to skip the time and effort-consuming task of
manually categorizing the articles ourselves. Once this dataset was
chosen and obtained, we would then clean and prepare it to be used to
train the classifiers.

\subsection{The dataset}
We gathered a dataset of news articles published in
\textit{Observador}\footnote{\url{http://observador.pt}}, one of the
main Portuguese newspapers, which stands out from the others for being
fairly young (it was created in May 2014) and for existing exclusively
online. The initial dataset comprised 42.475 entries, the most recent
ones dated from November 2016, from which we used only a subset, for
reasons later described.

We gathered all the categories used by Observador, and ordered them
from the most common to the least common. We selected the ones which
had more than 1000 articles in our dataset, and reduced our original
dataset to include articles from these categories only.
Figure~\ref{fig:articles} presents an overview of the selected
categories and the number of articles available for each one.

\begin{figure}[H]
\centerline{\includegraphics[width=.97\columnwidth]{imgs/articles}}
\caption{Total number of articles retrieved for each category}
\label{fig:articles}
\end{figure}

We them randomly selected, from each category, 700 articles to be used
to train the classifiers, and 200 to be used to evaluate their
performance.

For each article, we had available its contents (title, lead, body)
and several metadata fields (publication date, category, tags, etc). A
truncated JSON representation of an article can be found in
Listing~\ref{lst:article}.


\begin{lstlisting}[float,floatplacement=H,caption={Example of JSON representation of an article},label={lst:article}, extendedchars=true]
{
  Type: "sapo.obj.creativework.article",
  Source: {
      Name : "Observador"
  },
  Pretitle: "Benfica",
  Title: "Ruben Amorim com rotura total do ligamento cruzado",
  Author: {
      Name: "Observador"
  },
  Tags: [
      "benfica",
      "desporto",
      "futebol",
      "ruben amorim"
  ],
  PublishDate: ISODate("2014-08-25T18:33:00Z"),
  Lead: "Depois de Fejsa, mais uma baixa. O internacional português [...]",
  Body: "<p>O pior cenário confirmou-se. O Benfica informou esta segunda-feira [...]",
  URL: "http://observador.pt/2014/08/25/ruben-amorim-com-rotura-total-ligamento-cruzado/",
  CategoryPaths: ["Desporto"],
  Domain: "observador.pt",
  Language: "pt_PT",
  ...
}
\end{lstlisting}

\subsection{Preprocessing the articles}
\label{sec:preproc}

Originally, the dataset was obtained as a large MongoDB collection (more
than 2.5 million entries), containing articles from several Portuguese
and international newspapers. The process needed to transform this
collection into data our classifiers could process required querying
the database, exporting the news articles and splitting them into a
train and an evaluation datasets.

The database query selected articles from \texttt{Observador} where
the body had a length greater than 100 characters (to
discard some malformed articles which had an empty body or a body
composed of only a few words), and the categories included at least
one of the most common categories.

For each article returned by the query, the pretitle, title, subtitle
and lead fields, if present, were simply copied to a plain text file,
separated by blank lines. The body field, however, was stored in the
database in HTML format. As such, the HTML tags had to be stripped,
and then it was also added to the plain text file.

The files were stored in folders, separated by category. For each
category, 700 articles were allocated to the train set, and 200 to the
evaluation set. These preprocessing tasks were accomplished using Bash
and Node.js scripts.

Both datasets were loaded into R using the
\texttt{tm}\footnote{\url{https://cran.r-project.org/web/packages/tm/}}
package. Each was then passed to a function responsible for
preprocessing the text of the articles:

\begin{itemize}
    \item The text was converted to lowercase characters.
    \item The Portuguese stopwords were removed.
    \item Diacritics were converted to their normalized form (e.g.
        \framebox{à} $\rightarrow$ \framebox{a}).
    \item Punctuation signs were removed.
    \item Numbers were removed.
    \item Word were stemmed (e.g. \framebox{conseguiram} $\rightarrow$
        \framebox{consegu}).
    \item White space was removed and the text was tokenized.
\end{itemize}

A document-term matrix was calculated using the
\texttt{DocumentTermMatrix} method from the \texttt{tm} package.  For
each of the 5600 documents present in the train set the matrix included
all the terms which met the following requirements:

\begin{itemize}
    \item The term length was between 3 and 30 characters (to discard
        things like URLs or badly tokenized sentences).
    \item The term appeared in the document at least twice.
    \item The term appeared at least in 10 documents.
\end{itemize}
The algorithm used to weigh the terms in the matrix was
\textit{tf-idf}\cite{christopher2008introduction}. The obtained matrix
presented a sparsity of 99\% and contained 3234 distinct terms.

The final step was to remove the sparse terms from the matrix.
This allows to discard terms which might be too specific of the train
set and which might negatively influence the performance of the
classifiers by overfitting them to the train set. Additionally, a
smaller list of terms reduces the execution time of both training and
applying the classifier, and the memory footprint of the classifier.
However, while discarding sparse terms we might end up removing
relevant terms and increase our classifiers error rate.

We used the \texttt{removeSparseTerms} function from the \texttt{tm}
package, and produced three distinct lists of terms:
\begin{description}
    \item[-- $LT_{90}$] contained terms with 90\% or less sparsity.
    \item[-- $LT_{95}$] contained terms whose sparsity was under 95\%.
    \item[-- $LT_{99}$] contained the terms with a sparsity level
        below 99\%.
\end{description}

\subsection{Classification}

To create the classifiers, we used 5 well known algorithms: decision
tree (DT), k-nearest neighbors (KNN), naive Bayes (NB), neural network
(NN) and support vector
machine -- with radial kernel (SVM\textsubscript{RK}) and linear
kernel (SVM\textsubscript{LK}).

We trained each of the classifiers three different times, one for each
list of terms ($LT_{99}$, $LT_{95}$ and $LT_{90}$). Then we evaluated
each trained classifier using the test dataset.

The test dataset contained 1600 documents (200 belonging to each
category) and was preprocessed in a way similar to the train set
(described in the previous section,~\ref{sec:preproc}).  In each
iteration, however, the final list of terms in each test document was
restricted to terms present in the corresponding list of terms
($LT_{99}$, $LT_{95}$ and $LT_{90}$).

\section{Results}
A number of measurements and metrics were calculated regarding the
lists of terms, the classifiers training process and their results in
the evaluation process.

Table~\ref{tab:size} presents the size of each list of terms after
removing the terms whose sparsity was above the corresponding
threshold.

\begin{table}[H]
    \vspace{-.5cm}
    \caption{Lists size}
\begin{center}
\begin{tabular}{l|SS}
    & \parbox{6em}{\centering Sparsity\\threshold (\%)}  &
    \parbox{5em}{\centering Number of\\terms left}   \\\hline
    $LT_{90}$ &  90                 &  45 \\
    $LT_{95}$ &  95                 & 139 \\
    $LT_{99}$ &  99                 & 912 \\
\end{tabular}
\label{tab:size}
\end{center}
\end{table}

Table~\ref{tab:exectime} presents the execution time for training the
algorithm with the lowest error rate for each list of terms.

\begin{table}[H]
    \vspace{-.2cm}
    \caption{Execution times for training}
\begin{center}
\begin{tabular}{l|cr}
    & Algorithm  & \parbox{3em}{\centering Training\\time} \\\hline
    $LT_{90}$    & DT         & 12s  \\
    $LT_{95}$    & KNN        &  3m  \\
    $LT_{99}$    & KNN        & 57m  \\
\end{tabular}
\label{tab:exectime}
\end{center}
\end{table}


Table~\ref{tab:error} presents the error rates obtained for each list
of terms using each of the classifiers, with the value of the best
classifier for each list highlighted in bold.

\begin{table}[H]
    \vspace{-.2cm}
    \caption{Error rates}
\begin{center}
\begin{tabular}{l|rrrrrr}
                 & DT            & KNN           & NB   & NN   & SVM\textsubscript{RK} & SVM\textsubscript{LK} \\ \hline
    $LT_{90}$    & \textbf{0.60} & 0.71          & 0.85 & 0.61 & 0.88                  & 0.88                  \\
    $LT_{95}$    & 0.70          & \textbf{0.56} & 0.83 & 0.58 & 0.87                  & 0.87                  \\
    $LT_{99}$    & 0.70          & \textbf{0.35} & 0.87 & 0.76 & 0.87                  & 0.87                  \\
\end{tabular}
\label{tab:error}
\end{center}
\end{table}

Table~\ref{tab:confmat} presents the confusion matrix generated using
the k-nearest neighbors classifier with the $LT_{99}$ list of therms,
with the number of correct classifications for each category
highlighted in bold.

\begin{table}[H]
    \caption{Categories confusion matrix ($LT_{99}$ with KNN)}
\begin{center}
\begin{tabular}{l|rrrrrrrr}
& \rotatebox{45}{science}        & \rotatebox{45}{culture}   &
  \rotatebox{45}{sports}         & \rotatebox{45}{economics} &
  \rotatebox{45}{international}  & \rotatebox{45}{music}     &
  \rotatebox{45}{national}       & \rotatebox{45}{politics} \\\hline
science       & {\bf 127} & 19        & 1          & 6         & 12       & 24        & 11       & 0 \\
culture       & 5         & {\bf 102} & 2          & 2         & 3        & 79        & 6        & 1 \\
sports        & 1         & 3         & {\bf  168} & 7         & 3        & 14        & 4        & 0 \\
economics     & 5         & 3         & 0          & {\bf 146} & 5        & 17        & 14       & 10 \\
international & 12        & 12        & 7          & 17        & {\bf 90} & 35        & 16       & 11 \\
music         & 0         & 2         & 0          & 0         & 2        & {\bf 184} & 1        & 0 \\
national      & 9         & 9         & 9          & 19        & 15       & 31        & {\bf 88} & 21 \\
politics      & 1         & 3         & 0          & 30        & 8        & 17        & 14       & {\bf 127} \\
\end{tabular}
\label{tab:confmat}
\end{center}
\end{table}

\section{Discussion}

The analysis of the results obtained should take into account other
metrics besides the error rates obtained for each classifier.

The size of the list of terms, for example, gives us an idea of the
memory footprint of a classifier, a parameter which is of the utmost
importance if the goal is to implement a classifier as lightweight as
possible. The training time is also relevant, as shorter training
times give the possibility of retraining the classifiers more often,
allowing them to be updated as the corpus of articles grows in size.

Looking at the actual results obtained and represented in
Tables~\ref{tab:size} and \ref{tab:exectime} we can see that $LT_{90}$
has simultaneously the smallest list of terms (45) and the shortest
training time (12 seconds using the DT algorithm). However, $LT_{90}$
also presents the worst error rates, even when looking at the
algorithm which achieved its best results (0.60 using a DT
classifier).

On the opposite side, $LT_{99}$ presented the lowest error rates (0.35
using a k-nearest neighbors classifier), but it took almost one hour
to train and used a list comprising 912 terms.

The results obtained confirm that there is a tradeoff between the size
of the list of terms and the training time, on the one hand, and the
classifier error rate, on the other. However, a list of 912 terms
seems to be an acceptable memory footprint for a client side
classifier; additionally, the higher training time would not present
much problems in this scenario, as the classifier would be trained
beforehand and thus not be visible on the client side.

Given the reasonable values for the training time and the size of the
list of terms, and looking to the error rate obtained in each of tests
performed, we can conclude that the best option was to use the largest
list of terms ($LT_{99}$) and the KNN classifier.

It is worth noting that the categories of an article are not mutually
exclusive. In fact, an article can be classified as belonging to more
than one category. This might explain the greater error rates obtained
in the categories \textit{national} and  \textit{international}: one
might argue that these categories correspond less to the topic covered
by the article and are more related to the location of the news
content.

\section{Contributions and Future work}
For copyright reasons, the corpus used to train and evaluate the
classifiers described in this article cannot be shared. All the code
used to process the documents, to implement the classifiers and
evaluate them can be found at\\
\url{http://github.com/andrefs/mapi-msr-categorization}.

The tasks and results previously described already provide useful
insights into this matter. However, the lowest error rate obtained
(0.35) might still be improved upon, either by leveraging new
algorithms, fine tuning the ones already tested, or by increasing the
train corpus.

We have established that it is in fact possible
to develop a news article classifier which is lightweight enough to be
used (in real time) on the client side of a web application. The
following step will be the actual implementation of the classifier,
probably in the form of a JavaScript library.


\section*{Acknowledgment}

André Santos has a PhD scholarship from Fundação para a Ciência e
Tecnologia.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs03}
\bibliography{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
